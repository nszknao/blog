---
title: "プロジェクトのナレッジに特化したChatbotを作った話"
emoji: "🦙"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["LLM", "LangChain", "LlamaIndex"]
published: false
---

# tl:dr
- 社内のプロジェクトに特化したChatbotを作った話
- 技術的にはRetrieval-Augmented Generation（RAG）あたり
- xxすることで回答精度が向上したよ

# モチベーション
## 問い合わせに対応する時間的コストを減らしたい
Chatbot導入前に発生していた問題を洗い出すことで、どのような状態になりたいのかあるべき姿を明確にします。

弊社では社内メンバーやクライアントとのコミュニケーション手段として主にSlackを使っているのですが、プロジェクトの数が増えるにつれて問い合わせ対応にかける時間が増えてきました。

**TODO: 問い合わせ内容の3分類スライド**

問い合わせの内容は大きく3つに分類できます。
- プロダクトの仕様に対する質問
- 追加機能などの要望
- バグ報告

このうち後半の「要望」や「バグ報告」に関しては、ある程度フォーマットを定めることでその後のキャッチボールを減らすことができます。つまりAI導入云々の前に既存のツールを活用することで簡単にペインを解消できます。

ただ、最初の「質問」への応答に関しては一工夫が必要になります。（ドキュメントを確認してくださいと言うのは簡単ですが、クライアントに負担を強いることはできれば避けたい）

プロジェクトが大きくなればその分確認しないといけないドキュメントの分量も増えますし、全体を理解して説明できる人がPdMなど属人化するのを避けるためにも、何らかのシステマチックな解決策が必要だと考えました。

## RAGを使った仕組みを一通り作りたい
あとは技術者としての興味で、RAGでできることを肌感として理解しておきたかったからです😎


# 作ったもの
LlamaIndexの[ドキュメント](https://docs.llamaindex.ai/en/latest/getting_started/concepts.html)にRAGアプリケーションのステップについて記載があったので、その順番に沿って設計しました。

![RAGの開発ステップ](https://docs.llamaindex.ai/en/latest/_images/stages.png)
*RAGの開発ステップ[^1]*

## Loading
まず、外部のデータソースからRAGのパイプラインにデータを取り込みます。

プロジェクトで管理しているドキュメントのデータを使用しても良いのですが、最後のEvaluatingステップでパイプラインの評価を行いたいので、検証データもセットで提供されているパブリックデータを使いました。
https://huggingface.co/datasets/BeIR/fiqa

:::details FiQAデータセットについて by ChatGPT
「FiQA - Financial Opinion Mining and Question Answering」データセットは、金融関連のテキストデータを中心に構築されたデータセットで、意見マイニング（Opinion Mining）と質問応答（Question Answering）のタスクに特化しています。このデータセットは、金融分野における自然言語処理（NLP）の応用を促進することを目的としています。
:::

パブリックになっている日本語データセットは少ないです。英語のデータセットを利用する際に注意しなければならないのが、言語やドメインによって性能が悪化することがある[^2]ことです。そのため開発したパイプラインをプロジェクトに導入する前に試験運用は必要になりますが、まずは素早いイテレーションを回す仕組みを作る、と言う意味では活用する意味はあると思います。

またこのデータセットには元のコーパスも含まれているので、ユーザーからクエリされた質問を含むコンテキストをDBなどのソースデータから抽出するロジックもテストできました。以下の図でいう「relevant data」が抽出されたコンテキストです。

![RAGの全体像](https://docs.llamaindex.ai/en/latest/_images/basic_rag.png)
*RAGの全体像[^1]*

RAGの検証方法について調べる中で見つけたベンチマークに含まれるカラムはおおよそ以下のような構造でした。
- question
- answer
- contexts

質問に答えるときに利用したコンテキストをカラムに含めることで、検証の際に元のコーパス（たいていは膨大）を参照せずに済むと言うメリットは大きいと思います。ただ素人目にみたらこの「contexts」を抽出するのも難しいのでは？という疑問がありました。

RAGの全体像の図を見ても分かる通り、LLMにはプロンプトとしてqueryの他にrelevant dataしか与えられないため、ここの情報が適切に抽出できていることはレスポンスの精度に大きく影響します。そう言った意味でもFiQAのデータセットでは、コーパスの情報からcontextsを抽出するロジックも検証できるため安心感があります。

（私自身NLPの専門家でははないので、テキトーなことを言っていたらフィードバックいただければと思います🙇‍♂️）

## Indexing


## Storing

## Querying

## Evaluating

# まとめ
プロジェクトの中で決まったことをドキュメント化しておくことは今後より重要になるでしょう。

それではまたお会いしましょう！

[^1]: https://docs.llamaindex.ai/en/latest/getting_started/concepts.html
[^2]: https://www.promptingguide.ai/applications/synthetic_rag
